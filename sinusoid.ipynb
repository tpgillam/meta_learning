{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import dataclasses\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "from contexttimer import Timer\n",
    "from matplotlib import pyplot\n",
    "from more_itertools.recipes import pairwise\n",
    "from typing import Collection, NamedTuple, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class Batch:\n",
    "    \"\"\"Represent a batch of tasks, returned by `generate_sinusoid_batch`.\"\"\"\n",
    "    \n",
    "    x: numpy.ndarray  # (batch_size_meta, batch_size_inner)\n",
    "    y: numpy.ndarray  # (batch_size_meta, batch_size_inner)\n",
    "    amplitude: numpy.ndarray  # (batch_size_meta,)\n",
    "    phase: numpy.ndarray  # (batch_size_meta,)\n",
    "    input_range: Tuple[float, float]\n",
    "    amplitude_range: Tuple[float, float]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[1]\n",
    "        \n",
    "    def __getitem__(self, slice_):\n",
    "        \"\"\"Slice over `batch_size_inner`.\"\"\"\n",
    "        return Batch(\n",
    "            self.x[:, slice_],\n",
    "            self.y[:, slice_],\n",
    "            self.amplitude,\n",
    "            self.phase,\n",
    "            self.input_range,\n",
    "            self.amplitude_range\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_sinusoid_batch(\n",
    "        batch_size_meta: int,\n",
    "        batch_size_inner: int,\n",
    "        amplitude_range: Tuple[float, float] = (0.1, 5.0),\n",
    "        phase_range: Tuple[float, float] = (0., numpy.pi),\n",
    "        input_range: Tuple[float, float] = (-5.0, 5.0)) -> Batch:\n",
    "    \"\"\"Compute a batch of samples.\n",
    "    \n",
    "    We draw `batch_size_meta` tasks, and for each task a batch of `batch_size_inner` points. Each \"task\"\n",
    "    represents a regression problem, underlied by a sine wave with some amplitude and phase.\n",
    "    \n",
    "    Args:\n",
    "        batch_size_meta: The number of tasks to draw.\n",
    "        batch_size_inner: The number of samples for each task.\n",
    "        amplitude_range: Draw the amplitude of the sine wave for the task uniformly from this range.\n",
    "        phase_range: Draw the phase of the sine wave for the task uniformly from this range.\n",
    "        input_range: The range from which the input variable will be drawn uniformly.\n",
    "    \"\"\"\n",
    "    amplitude = numpy.random.uniform(amplitude_range[0], amplitude_range[1], batch_size_meta)\n",
    "    phase = numpy.random.uniform(phase_range[0], phase_range[1], batch_size_meta)\n",
    "    \n",
    "    # All input locations are independent.\n",
    "    x = numpy.random.uniform(\n",
    "        input_range[0],\n",
    "        input_range[1],\n",
    "        (batch_size_meta, batch_size_inner))\n",
    "    \n",
    "    # To compute the outputs, we should broadcast the amplitude & phase over all inner samples.\n",
    "    y = numpy.expand_dims(amplitude, axis=1) * numpy.sin(x - numpy.expand_dims(phase, axis=1))\n",
    "    \n",
    "    return Batch(x, y, amplitude, phase, input_range, amplitude_range)\n",
    "\n",
    "\n",
    "def plot_task(batch: Batch, index: int):\n",
    "    \"\"\"Plot the task with the specified `index` from the given `batch`.\"\"\"\n",
    "    # Plot the reference curve\n",
    "    truth_x = numpy.linspace(batch.input_range[0], batch.input_range[1], 200)\n",
    "    truth_y = batch.amplitude[index] * numpy.sin(truth_x - batch.phase[index])\n",
    "    pyplot.plot(truth_x, truth_y, c='r', label='truth')\n",
    "    \n",
    "    # Plot the sample points.\n",
    "    pyplot.plot(batch.x[index], batch.y[index], '^', label='samples')\n",
    "    \n",
    "    pyplot.xlim(*batch.input_range)\n",
    "    max_y = max(batch.amplitude_range)\n",
    "    pyplot.ylim(-max_y, max_y)\n",
    "    \n",
    "    pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_sinusoid_batch(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_task(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99536483,  2.2299763 ,  4.70289719,  3.21569457,  0.27551067],\n",
       "       [-1.6852327 , -1.46017804, -4.20969699,  0.5591438 , -3.34205537],\n",
       "       [-2.04762169,  3.40608485, -1.37437991, -1.59186022,  2.10401744]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalRegressor(torch.nn.Module):\n",
    "    \"\"\"A module suitable for producing  \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes: Collection[int] = (40, 40)):  \n",
    "        super().__init__()\n",
    "        dim_in = 1\n",
    "        dim_out = 1\n",
    "        all_dims = (dim_in,) + tuple(hidden_sizes) + (dim_out,)\n",
    "        self.layers = [torch.nn.Linear(dim_1, dim_2) for dim_1, dim_2 in pairwise(all_dims)]\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # In the reference implementation, initialisation is:\n",
    "            #    * zero for bias terms\n",
    "            #    * truncated_normal, std=0.01 for weights.\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "            # TODO truncated normal not available...\n",
    "            torch.nn.init.xavier_normal_(layer.weight)\n",
    "            \n",
    "            # Register this as a sub-module, so we declare the existence of the necessary parameters\n",
    "            self.add_module(f'layer_{i}', layer)\n",
    "            \n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            \n",
    "            # We should not apply the non-linearity on the last layer!\n",
    "            is_last = i == len(self.layers) - 1\n",
    "            if not is_last:\n",
    "                x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Standard device on which to run computations.\"\"\"\n",
    "    return torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From code:\n",
    "batch_size_meta = 25\n",
    "\n",
    "# The inner batch size. It seems to vary between plots.\n",
    "# We actually generate *twice* this amount of data in total; half is used for the inner\n",
    "# training loop, and the other half for inner testing.\n",
    "batch_size_inner = 10\n",
    "\n",
    "learning_rate_meta = 0.001  # Meta Adam optimizer\n",
    "learning_rate_update = 0.001  # Inner Adam optimizer\n",
    "num_steps_train_meta = 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def model_eval(model):\n",
    "    \"\"\"Enter 'evaluation mode' for the given model. \"\"\"\n",
    "    model.eval()  # DANGER: This *mutates* the model. It returns a reference to itself\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _module_device(module):\n",
    "    \"\"\"Find the pytorch device to which the given module is bound.\"\"\"\n",
    "    devices = {x.device for x in module.parameters()}\n",
    "    if len(devices) != 1:\n",
    "        raise ValueError(f\"Found candidate devices {devices}\")\n",
    "    return next(iter(devices))\n",
    "\n",
    "\n",
    "def _train_step(module, train_batch: Batch, optimizer):\n",
    "    device = _module_device(module)\n",
    "    train_x = torch.tensor(train_batch.x.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    train_y = torch.tensor(train_batch.y.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    train_y_pred = module(train_x)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = mse_loss(train_y_pred, train_y)\n",
    "\n",
    "    # Zero gradients, perform a backward pass (compute gradients wrt. the loss), and update the \n",
    "    # weights accordingly.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "class BatchPrediction(NamedTuple):\n",
    "    y: numpy.ndarray\n",
    "    mse_loss: float\n",
    "\n",
    "\n",
    "def _get_test_prediction(module, test_batch: Batch) -> BatchPrediction:\n",
    "    \"\"\"Get the prediction when evaluating the module on the given batch.\"\"\"\n",
    "    device = _module_device(module)\n",
    "    test_x = torch.tensor(test_batch.x.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    test_y = torch.tensor(test_batch.y.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    # No need to accumulate gradients when evaluating the validation loss.\n",
    "    # Also, we put the model into \"evaluation mode\" for the purpose of computing the prediction.\n",
    "    # This is to prevent layers like BatchNorm / Dropout mutating their internal state.\n",
    "    with torch.no_grad(), model_eval(module):\n",
    "        test_y_pred = module(test_x)\n",
    "        test_mse = mse_loss(test_y_pred, test_y).item()\n",
    "        \n",
    "    return BatchPrediction(test_y_pred.cpu().numpy().squeeze(1), test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_example(\n",
    "        *,\n",
    "        num_steps_train: int = 70000,\n",
    "        batch_size_meta: int = 25,\n",
    "        batch_size_inner: int = 10,\n",
    "        learning_rate: float = 0.001,\n",
    "        verbose: bool = False) -> list:\n",
    "    device = get_device()\n",
    "    module = SinusoidalRegressor().to(device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(module)\n",
    "\n",
    "    # Construct an Optimizer. The call to model.parameters() in the constructor will contain the\n",
    "    # learnable parameters of the module.\n",
    "    optimizer = torch.optim.Adam(module.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # These are the MSEs from the 'test' part of the batches drawn within the meta-training process.\n",
    "    meta_train_test_mses = []\n",
    "\n",
    "    numpy.random.seed(42)\n",
    "    \n",
    "    with Timer() as timer:\n",
    "        for i_meta in range(num_steps_train):\n",
    "            \n",
    "            # Generate a batch with which to train.\n",
    "            batch = generate_sinusoid_batch(batch_size_meta, batch_size_inner * 2)\n",
    "            \n",
    "            train_batch = batch[:batch_size_inner]\n",
    "            test_batch = batch[batch_size_inner:]\n",
    "            \n",
    "            _train_step(module, train_batch, optimizer)\n",
    "\n",
    "            if i_meta % 500 == 0:\n",
    "                prediction = _get_test_prediction(module, test_batch)\n",
    "                meta_train_test_mses.append(prediction.mse_loss)\n",
    "                if verbose:\n",
    "                    print(f'Epoch {i_meta + 1}/{num_steps_train}:   {prediction.mse_loss}')\n",
    "\n",
    "    print(timer)\n",
    "    \n",
    "    return meta_train_test_mses, module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_test_mses, module = pretrain_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(meta_train_test_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_test(\n",
    "        module,\n",
    "        *,\n",
    "        batch_size_inner: int = 10,\n",
    "        learning_rate: float = 0.001):\n",
    "    \"\"\"Perform a single test on the trained module\"\"\"\n",
    "    # FIXME We need to make an independent copy of the module's parameters before we mutate them in training.\n",
    "    batch = generate_sinusoid_batch(1, 2 * batch_size_inner)\n",
    "    train_batch = batch[:batch_size_inner]\n",
    "    test_batch = batch[batch_size_inner:]\n",
    "    \n",
    "    prediction = _get_test_prediction(module, test_batch)\n",
    "    plot_task(train_batch, 0)\n",
    "    pyplot.plot(test_batch.x[0], prediction.y, '<')\n",
    "    pyplot.show()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(module.parameters(), lr=learning_rate)\n",
    "    \n",
    "    _train_step(module, train_batch, optimizer)\n",
    "    #TODO Evaluate over a much finer grid to make a better plot\n",
    "    prediction = _get_test_prediction(module, test_batch)\n",
    "    plot_task(train_batch, 0)\n",
    "    pyplot.plot(test_batch.x[0], prediction.y, '<')\n",
    "    print(prediction.y)\n",
    "    pyplot.show()\n",
    "    \n",
    "    for _ in range(29):\n",
    "        _train_step(module, train_batch, optimizer)\n",
    "    prediction = _get_test_prediction(module, test_batch)\n",
    "    plot_task(train_batch, 0)\n",
    "    pyplot.plot(test_batch.x[0], prediction.y, '<')\n",
    "    print(prediction.y)\n",
    "    pyplot.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(33)\n",
    "meta_test(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module(torch.from_numpy(numpy.expand_dims(numpy.array([-3, -2, -1, 0, 4], dtype=numpy.float32), 1)).to(get_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
